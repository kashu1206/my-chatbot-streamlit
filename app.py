import base64 # <-- ã“ã‚ŒãŒä»¥å‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã‹ã£ãŸå•é¡Œã«å¯¾ã™ã‚‹ä¿®æ­£ï¼ˆä»Šå›žã¯æ—¢ã«ä¿®æ­£æ¸ˆã¿ã¨ä»®å®šï¼‰
import streamlit as st
import time
import google.generativeai as genai
import io
import os
import json
import tempfile

# GCP Speech-to-Text and Text-to-Speech clients
from google.cloud import speech_v1p1beta1 as speech
from google.cloud import texttospeech_v1 as texttospeech
from google.oauth2 import service_account

# VAD (Voice Activity Detection) libraries
from pydub import AudioSegment
from pydub.silence import detect_nonsilent

try:
    from streamlit_mic_recorder import mic_recorder
except ImportError:
    st.error("`streamlit-mic_recorder` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install streamlit-mic-recorder` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    mic_recorder = None

# --- 0. ç’°å¢ƒå¤‰æ•°ã®è¨­å®šã¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ– ---

# Gemini API Key
try:
    gemini_api_key = st.secrets["GEMINI_API_KEY"]
    genai.configure(api_key=gemini_api_key)
except KeyError:
    st.error("Error: GEMINI_API_KEY is not set in Streamlit Secrets.")
    st.stop() # Gemini APIã‚­ãƒ¼ãŒãªã‘ã‚Œã°ã‚¢ãƒ—ãƒªã‚’åœæ­¢

# --- GCP Credentials (for Speech-to-Text and Text-to-Speech) ---
_can_use_gcp_voice = False
_speech_client = None
_texttospeech_client = None
_decoded_gcp_credentials_json_string = None 
_temp_key_file_path = None # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

try:
    # å„ªå…ˆ: Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸèªè¨¼æƒ…å ±ã‚’ç¢ºèª
    if "GCP_CREDENTIALS_BASE64" in st.secrets:
        encoded_credentials = st.secrets["GCP_CREDENTIALS_BASE64"]
        _decoded_gcp_credentials_json_string = base64.b64decode(encoded_credentials.encode("utf-8")).decode("utf-8")
        st.success("GCP Credentials loaded successfully from Base64 secret!")

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›´æŽ¥GCP_CREDENTIALSãŒJSONæ–‡å­—åˆ—ã¨ã—ã¦è¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
    elif "GCP_CREDENTIALS" in st.secrets:
        raw_credentials = st.secrets["GCP_CREDENTIALS"]
        if isinstance(raw_credentials, dict): # æ—¢ã«è¾žæ›¸åž‹ã®å ´åˆ
            _decoded_gcp_credentials_json_string = json.dumps(raw_credentials)
        else: # æ–‡å­—åˆ—ã®å ´åˆ
            _decoded_gcp_credentials_json_string = raw_credentials
        st.success("GCP Credentials loaded successfully from direct secret!")
    
    else:
        st.warning("Warning: GCP_CREDENTIALS (Base64 or direct) for Speech-to-Text/Text-to-Speech are not set in Streamlit Secrets. Voice input/output will not be available.")
        # GCPèªè¨¼æƒ…å ±ãŒãªã„å ´åˆã§ã‚‚Geminiéƒ¨åˆ†ã¯å‹•ãã‚ˆã†ã« st.stop() ã¯å‘¼ã°ãªã„
        
    # èªè¨¼æƒ…å ±æ–‡å­—åˆ—ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ã€GCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
    if _decoded_gcp_credentials_json_string:
        # ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæƒ…å ±ã§èªè¨¼æƒ…å ±ã‚’ç”Ÿæˆ
        _gcp_credentials_info = json.loads(_decoded_gcp_credentials_json_string)
        credentials = service_account.Credentials.from_service_account_info(_gcp_credentials_info)
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦JSONã‚’ä¿å­˜ã—ã€GOOGLE_APPLICATION_CREDENTIALS ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
        # ã“ã‚Œã¯ã€`service_account.Credentials` ã‚’ç›´æŽ¥æ¸¡ã™å ´åˆã§ã‚‚ã€
        # ä»–ã®Google Cloudãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒç’°å¢ƒå¤‰æ•°ã‚’å‚ç…§ã™ã‚‹å¯èƒ½æ€§ã«å‚™ãˆã‚‹ãŸã‚ã§ã™ã€‚
        with tempfile.NamedTemporaryFile(mode="w", delete=False, encoding="utf-8") as temp_key_file:
            temp_key_file.write(_decoded_gcp_credentials_json_string)
            _temp_key_file_path = temp_key_file.name
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = _temp_key_file_path

        # GCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        _speech_client = speech.SpeechClient(credentials=credentials)
        _texttospeech_client = texttospeech.TextToSpeechClient(credentials=credentials)
        _can_use_gcp_voice = True
        st.info("GCP Speech-to-Text and Text-to-Speech clients initialized.")

except Exception as e:
    st.error(f"Critical error during GCP credentials setup: {e}")
    st.warning("Voice input/output will not be available due to GCP client initialization failure.")
    _can_use_gcp_voice = False

# --- ã‚¢ãƒ—ãƒªçµ‚äº†æ™‚ã«ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— ---
# Streamlitã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã¨tempfileã®å‰Šé™¤ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã¯è¤‡é›‘ã§ã™ãŒã€
# Streamlit Cloudã§ã¯ã‚¢ãƒ—ãƒªã®å†èµ·å‹•æ™‚ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ãŒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚Œã‚‹ãŸã‚ã€
# æ˜Žç¤ºçš„ãª os.remove(_temp_key_file_path) ã¯å¿…é ˆã§ã¯ãªã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚
# ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œã—ã¦ã„ã¦ã€ç¢ºå®Ÿã«å‰Šé™¤ã—ãŸã„å ´åˆã¯è€ƒæ…®ã—ã¾ã™ã€‚

# --- éŸ³å£°å‡¦ç† (ç„¡éŸ³æ¤œå‡ºãƒ»ãƒˆãƒªãƒŸãƒ³ã‚°) ã®è¨­å®š ---
SAMPLE_RATE = 16000  # Streamlit mic recorder ã¯é€šå¸¸16kHzã§éŒ²éŸ³ã•ã‚Œã‚‹ (GCP Speech-to-Textã®æŽ¨å¥¨)

# --- éŸ³å£°ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã¸ (Speech-to-Text) ---
def transcribe_audio_gcp(audio_bytes):
    if not _speech_client:
        st.error("Speech-to-Text client is not initialized.")
        return ""

    try:
        # pydubã§ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒã‚¤ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ (streamlit-mic-recorderã¯é€šå¸¸webmå½¢å¼ã§å‡ºåŠ›)
        audio_segment = AudioSegment.from_file(io.BytesIO(audio_bytes), format="webm")
        
        # 16kHz, 1ãƒãƒ£ãƒ³ãƒãƒ«ã«å¤‰æ› (GCP Speech-to-Textã®æŽ¨å¥¨)
        if audio_segment.frame_rate != SAMPLE_RATE or audio_segment.channels != 1:
            audio_segment = audio_segment.set_frame_rate(SAMPLE_RATE).set_channels(1)
        
        # pydub.silence.detect_nonsilent ã‚’ä½¿ç”¨ã—ã¦ç„¡éŸ³åŒºé–“ã‚’æ¤œå‡ºãƒ»ãƒˆãƒªãƒŸãƒ³ã‚°
        nonsilent_chunks = detect_nonsilent(audio_segment, 
                                            min_silence_len=500, # 500msä»¥ä¸Šã®ç„¡éŸ³ã‚’æ¤œå‡º
                                            silence_thresh=-35)  # -35dBFSä»¥ä¸‹ã®éŸ³é‡ã‚’ç„¡éŸ³ã¨åˆ¤å®š

        if not nonsilent_chunks: # éŸ³å£°ãŒå…¨ãæ¤œå‡ºã•ã‚Œãªã‹ã£ãŸå ´åˆ
            st.info("No substantial speech detected after trimming.")
            return ""

        # ç„¡éŸ³ã§ãªã„ãƒãƒ£ãƒ³ã‚¯ã®ã¿ã‚’çµåˆ
        trimmed_audio = AudioSegment.empty()
        for start_ms, end_ms in nonsilent_chunks:
            trimmed_audio += audio_segment[start_ms:end_ms]

        st.info(f"Original audio duration: {len(audio_segment)/1000:.2f}s, Trimmed audio duration: {len(trimmed_audio)/1000:.2f}s")

        # --- ã“ã“ã‹ã‚‰ä¿®æ­£ ---
        # pydub.AudioSegment ã®ã‚µãƒ³ãƒ—ãƒ«å¹…ã‚’16-bit (2ãƒã‚¤ãƒˆ) ã«è¨­å®š
        # GCP Speech-to-Text ãŒ LINEAR16 (16-bit PCM) ã‚’è¦æ±‚ã™ã‚‹ãŸã‚
        trimmed_audio = trimmed_audio.set_sample_width(2) # 2 bytes = 16 bits
        # --- ä¿®æ­£ã“ã“ã¾ã§ ---

        # å†ã³ãƒã‚¤ãƒˆåˆ—ã«å¤‰æ› (WAVå½¢å¼ã§ãƒ˜ãƒƒãƒ€ã‚’ä»˜ä¸Žã—ã¦é€ã‚‹ã®ãŒæœ€ã‚‚ç¢ºå®Ÿ)
        trimmed_audio_bytes = trimmed_audio.export(format="wav").read()

        audio = speech.RecognitionAudio(content=trimmed_audio_bytes)
        config = speech.RecognitionConfig(
            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16, # WAVãªã®ã§LINEAR16
            sample_rate_hertz=SAMPLE_RATE,
            language_code="en-US", # è‹±èªžã§èªè­˜
            enable_automatic_punctuation=True, # è‡ªå‹•å¥èª­ç‚¹
        )

        response = _speech_client.recognize(config=config, audio=audio)
        transcript = ""
        for result in response.results:
            transcript += result.alternatives[0].transcript
        return transcript
    except Exception as e:
        st.error(f"Error transcribing audio with Google Cloud Speech-to-Text API: {e}")
        return ""

# --- ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°ã¸ (Text-to-Speech) ---
def synthesize_text_gcp(text):
    if not _texttospeech_client:
        st.error("Text-to-Speech client is not initialized.")
        return None

    try:
        synthesis_input = texttospeech.SynthesisInput(text=text)
        voice = texttospeech.VoiceSelectionParams(
            language_code="en-US", # è‹±èªžéŸ³å£°
            name="en-US-Standard-F", # æ¨™æº–çš„ãªå¥³æ€§ã®å£° (å¿…è¦ã«å¿œã˜ã¦"en-US-Wavenet-F"ãªã©ã‚‚æ¤œè¨Ž)
            ssml_gender=texttospeech.SsmlVoiceGender.FEMALE
        )
        audio_config = texttospeech.AudioConfig(
            audio_encoding=texttospeech.AudioEncoding.MP3,
            speaking_rate=1.0, # è©±é€Ÿ (1.0ãŒæ¨™æº–)
        )

        response = _texttospeech_client.synthesize_speech(
            input=synthesis_input, voice=voice, audio_config=audio_config
        )
        return response.audio_content # MP3ãƒã‚¤ãƒˆåˆ—
    except Exception as e:
        st.error(f"Error synthesizing speech with Google Cloud Text-to-Speech API: {e}")
        return None

# --- Geminiãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ– ---
@st.cache_resource
def get_gemini_model():
    return genai.GenerativeModel('gemini-pro')
model = get_gemini_model()

# --- ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šã¨ãƒ¬ãƒ™ãƒ«èª¿æ•´æ©Ÿèƒ½ ---
def get_system_instruction(level):
    # Base instruction common to all levels, explicitly enforcing English responses
    base_instruction = (
    "You are an English conversation partner who helps users improve their English skills. "
    "You are also an experienced English teacher with extensive experience guiding native Japanese speakers in learning English as a foreign language. "
    "Please keep in mind that the user is a native Japanese speaker throughout your interactions. "
    "**Always respond only in English. Do not use Japanese at all.**"
)

    if level == "Hana":
        return base_instruction + (
            " Your name is Tanaka Hana. You are a girl from Wakaba Junior High School, originally from Wakaba City."
            " You have a gentle and meticulous personality, and your friends often consult you when they're in trouble."
            " You've been dedicated to soccer since age 3. Recently, you've been enjoying family camping trips and mastering camp cooking."
            " You're preparing to play in an overseas soccer league after junior high school graduation."
            " Your favorite subject is English, and your hobbies are soccer and baking sweets."
            " You will converse according to the English ability of a Japanese junior high school 1st grader."
            " Focus on basic vocabulary like 'be, have, go, see, eat, school, friend, happy, kind, clean, big, small', targeting a total vocabulary of around 300-1300 words."
            " Speak slowly using very simple words and short sentences (maximum 10 words per sentence)."
            " Ask simple questions to encourage conversation."
            " **Do not point out any grammar or spelling mistakes in the user's input. Accept them as they are and continue the conversation.**"
        )
    elif level == "Mark":
        return base_instruction + (
            " Your name is Mark Davis. You are a boy from Wakaba Junior High School, originally from Seattle, USA."
            " You have a cheerful personality and are a mood-maker in class. You have an older sister who is in high school."
            " You love interacting with people and have been entrusted with looking after the new first-year students in your basketball club."
            " While continuing your beloved basketball, you are diligently studying to become a veterinarian."
            " Your favorite subject is Science, and you are very athletic, placing high in the Wakaba Marathon every year."
            " You will converse according to the English ability of a Japanese junior high school graduate (Eiken Grade 3 equivalent)."
            " Use everyday, emotional, and regional vocabulary such as 'enjoy, plan, decide, describe, delicious, exciting, important, healthy, wonderful, popular', targeting a total vocabulary of around 1250-2100 words."
            " **Only if there are obvious grammar or spelling mistakes in the user's input, gently point them out or suggest a more natural way to phrase it, assisting the user to correct them on their own.**"
            " Incorporate slightly longer sentences and somewhat complex sentence structures, focusing on a natural flow of conversation."
        )
    elif level == "Ms. Brown":
        return base_instruction + (
            " Your name is Ms. Lucy Brown. You are an ALT (Assistant Language Teacher) at Wakaba Junior High School, originally from London, UK."
            " You love reading and own many different books. Recently, you've been reading a lot of Japanese novels."
            " When you were a junior high school student, your dream was to be a novelist, and you often wrote novels based on everyday events."
            " You love houseplants and animals."
            " You will converse according to the English ability of a Japanese English teacher (Eiken Pre-1st Grade, TOEFL PBT 550+, CBT 213+, iBT 80+, TOEIC 730+)."
            " Use professional and abstract vocabulary suitable for university-level studies, specifically targeting words like 'accommodate, acknowledge, eliminate, prohibit, uphold, magnify, acquisition, curriculum, literacy, heritage, ailment, revenue', with a total vocabulary of around 7500-9000 words."
            " If there are grammar or spelling mistakes in the user's input, **gently point them out or suggest more sophisticated expressions, assisting the user to think and correct them on their own.**"
            " However, your role is primarily a facilitator, encouraging the user's critical thinking and expression. Discuss a wide range of topics deeply and in natural English."
        )
    else: # Default case, though unlikely with selectbox
        return base_instruction + " Use natural, everyday English. Engage in friendly conversation and ask open-ended questions."


# --- Streamlit UIã®æ§‹ç¯‰ ---
st.set_page_config(layout="wide")
st.title("English Conversation Partner ðŸ—£ï¸")
st.write("Let's practice English together!")

with st.sidebar:
    st.header("Settings")
    
    # è‹±èªžãƒ¬ãƒ™ãƒ«é¸æŠž
    english_level = st.selectbox(
        "Select your English Level:",
        [
            "Hana",
            "Mark",
            "Ms. Brown"
        ],
        index=0,
        key="english_level_selector"
    )

    # éŸ³å£°å…¥åŠ›/å‡ºåŠ›ã®ON/OFFãƒˆã‚°ãƒ« (GCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã§ããŸå ´åˆã®ã¿æœ‰åŠ¹)
    use_audio_io = st.toggle("Enable Voice Input/Output (GCP)", value=False, key="audio_io_toggle", disabled=not _can_use_gcp_voice)

    if use_audio_io and (mic_recorder is None or not _can_use_gcp_voice):
        st.warning("éŸ³å£°å…¥å‡ºåŠ›ã¯ã€`streamlit-mic-recorder` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä¸è¶³ã—ã¦ã„ã‚‹ã‹ã€GCPèªè¨¼æƒ…å ±ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ç„¡åŠ¹ã§ã™ã€‚")
    elif not use_audio_io:
        st.info("éŸ³å£°å…¥å‡ºåŠ›ã¯ç¾åœ¨ç„¡åŠ¹ã§ã™ã€‚è¨­å®šã§æœ‰åŠ¹ã«ã§ãã¾ã™ã€‚")

    st.info("The AI will always respond in English, based on your selected level.")

# --- ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ– (ãƒ¬ãƒ™ãƒ«é¸æŠžã«å¿œã˜ã¦ system_instruction ã‚’è¨­å®š) ---
current_system_instruction = get_system_instruction(english_level)
model = genai.GenerativeModel(
    'gemini-flash-latest',
    system_instruction=current_system_instruction
)

# --- ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’Streamlitã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã§ç®¡ç† ---
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.previous_english_level = english_level # Initialize previous_english_level
    # åˆå›žãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ
    initial_message = ""
    if english_level == "Hana":
        initial_message = "Hi! I'm Tanaka Hana. What would you like to talk about today?"
    elif english_level == "Mark":
        initial_message = "Hey there! I'm Mark. What's up?"
    elif english_level == "Ms. Brown":
        initial_message = "Good day! I'm Ms. Brown. How may I assist you today?"
    st.session_state.messages.append({"role": "assistant", "content": initial_message})

    # åˆå›žãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®éŸ³å£°å†ç”Ÿ
    if use_audio_io and _can_use_gcp_voice and initial_message:
        audio_output = synthesize_text_gcp(initial_message)
        if audio_output:
            st.audio(audio_output, format="audio/mp3", autoplay=True)


if st.session_state.get("previous_english_level") != english_level:
    st.session_state.messages = []
    # Dynamic initial message after level change
    initial_message = "Hello! Let's start our conversation. What's on your mind today?"
    if english_level == "Hana":
        initial_message = "Hi! I'm Tanaka Hana. What would you like to talk about today?"
    elif english_level == "Mark":
        initial_message = "Hey there! I'm Mark. What's up?"
    elif english_level == "Ms. Brown":
        initial_message = "Good day! I'm Ms. Brown. How may I assist you today?"

    system_change_message = f"Okay, switching to the {english_level} . {initial_message}"
    st.session_state.messages.append({"role": "assistant", "content": system_change_message})
    st.session_state.previous_english_level = english_level

    # ãƒ¬ãƒ™ãƒ«å¤‰æ›´æ™‚ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®éŸ³å£°å†ç”Ÿ
    if use_audio_io and _can_use_gcp_voice and system_change_message:
        audio_output = synthesize_text_gcp(system_change_message)
        if audio_output:
            st.audio(audio_output, format="audio/mp3", autoplay=True)

# --- æ—¢å­˜ã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¡¨ç¤º ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ã‚’å—ã‘ä»˜ã‘ã‚‹ ---
user_input_prompt = ""
if use_audio_io:
    st.write("Click the mic and speak!")
    audio_bytes = None
    if mic_recorder: # mic_recorder ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
        recorded_audio = mic_recorder(
            start_prompt="ðŸŽ¤ Start recording",
            stop_prompt="â¹ï¸ Stop recording",
            just_once=True,
            use_container_width=True,
            key='user_mic_input'
        )
        if recorded_audio:
            audio_bytes = recorded_audio['bytes']

    if audio_bytes and _can_use_gcp_voice:
        with st.spinner("Processing audio and transcribing..."):
            user_input_prompt = transcribe_audio_gcp(audio_bytes)
            if user_input_prompt:
                st.write(f"You said: {user_input_prompt}")
            else:
                st.warning("Could not transcribe audio. Please try speaking clearer.")
    
    # éŸ³å£°å…¥åŠ›ãŒæˆåŠŸã—ãªã‹ã£ãŸå ´åˆã€ã¾ãŸã¯éŸ³å£°å…¥åŠ›ãŒç„¡åŠ¹ãªå ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ã‚’è¡¨ç¤º
    if not user_input_prompt: # user_input_prompt ãŒç©ºã®å ´åˆ
        user_input_prompt = st.chat_input("Start practicing English with me! (Or use mic above)", disabled=bool(audio_bytes))

else: # use_audio_io ãŒ False ã®å ´åˆ
    user_input_prompt = st.chat_input("Start practicing English with me! (Type here)")


if user_input_prompt:
    st.session_state.messages.append({"role": "user", "content": user_input_prompt})
    with st.chat_message("user"):
        st.markdown(user_input_prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        gemini_chat_history = []
        for msg in st.session_state.messages:
            if msg["role"] == "user":
                gemini_chat_history.append({"role": "user", "parts": [msg["content"]]})
            elif msg["role"] == "assistant":
                # ãƒ¬ãƒ™ãƒ«å¤‰æ›´æ™‚ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯å±¥æ­´ã«å«ã‚ãªã„
                if "Okay, switching to the " not in msg["content"]: 
                    gemini_chat_history.append({"role": "model", "parts": [msg["content"]]})


        chat = model.start_chat(history=gemini_chat_history)

        try:
            response_generator = chat.send_message(user_input_prompt, stream=True)

            for chunk in response_generator:
                full_response += chunk.text
                message_placeholder.markdown(full_response + "â–Œ")
                time.sleep(0.05)
            message_placeholder.markdown(full_response)

            st.session_state.messages.append({"role": "assistant", "content": full_response})

            # Assistantã®è¿”ç­”ã‚’éŸ³å£°ã§å†ç”Ÿ
            if use_audio_io and _can_use_gcp_voice and full_response:
                audio_output = synthesize_text_gcp(full_response)
                if audio_output:
                    st.audio(audio_output, format="audio/mp3", autoplay=True)

        except Exception as e:
            st.error(f"An error occurred with Gemini: {e}. Please try again.")
            st.session_state.messages.append({"role": "assistant", "content": f"An error occurred with Gemini: {e}. Please try again."})